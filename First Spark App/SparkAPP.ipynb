{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c01e424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5323dee",
   "metadata": {},
   "source": [
    "### SparkSession is the entry point to programming Spark with the Dataset and DataFrame API. It can be used create DataFrame, register DataFrame as tables, execute SQL over tables, cache tables, and read parquet files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b1b710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating SparkSession\n",
    "spark = SparkSession.builder.appName(\"FirstApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44921045",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Schema for your Dataframe\n",
    "myschema = StructType([\n",
    "    StructField(\"UserID\",IntegerType(),True),\n",
    "    StructField(\"Name\",StringType(),True),\n",
    "    StructField(\"Age\",IntegerType(),True),\n",
    "    StructField(\"Friends\",IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f7a83",
   "metadata": {},
   "source": [
    "### Method 1\n",
    "If your data already has column names and their data types properly defined within the file, you may not necessarily need \n",
    "to create a schema explicitly in your code. In this case, Spark can perform automatic schema inference, where it examines the\n",
    "first row of the CSV file to determine the column names and attempts to infer the data types of each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82df3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = spark.read.csv(\"name.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99592561",
   "metadata": {},
   "source": [
    "### Method 2\n",
    "There are some situations where explicitly defining a schema is still beneficial:\n",
    "1) Data Type Control <br>\n",
    "2) Data Validation <br>\n",
    "3) Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee6ba8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Dataframe from a csv file and load into Spark Dataframe\n",
    "people = spark.read.format(\"csv\")\\\n",
    "    .schema(myschema)\\\n",
    "    .option(\"path\",\"fakefriends.csv\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd24c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserID: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Friends: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dab0ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Columns   : ['UserID', 'Name', 'Age', 'Friends']\n",
      "Column Dtypes : [('UserID', 'int'), ('Name', 'string'), ('Age', 'int'), ('Friends', 'int')]\n"
     ]
    }
   ],
   "source": [
    "print(\"All Columns   :\",people.columns)\n",
    "print(\"Column Dtypes :\",people.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8035c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               Age|\n",
      "+-------+------------------+\n",
      "|  count|               500|\n",
      "|   mean|            43.708|\n",
      "| stddev|14.864340996711995|\n",
      "|    min|                18|\n",
      "|    max|                69|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(['Age']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abe35523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    Name|Age|\n",
      "+--------+---+\n",
      "|    Will| 33|\n",
      "|Jean-Luc| 26|\n",
      "+--------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "people.select(['Name','Age']).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "413292c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing all thetransformations\n",
    "output = people.select(people.UserID,people.Name,people.Age,people.Friends)\\\n",
    "         .where(people.Age < 30).withColumn('insert_ts', func.current_timestamp())\\\n",
    "         .orderBy(people.UserID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8164f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop a column\n",
    "# output.drop('column_name')\n",
    "# Rename a column\n",
    "# output.withColumnRenamed('Name','New Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29dbcc56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Count of output DataFrame\n",
    "output.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d2c1a",
   "metadata": {},
   "source": [
    "### Temp View is used when you wanted to store the table for a specific spark session. Once created you can use it to run SQL queries. These views are session-scoped i.e valid only that running spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12be31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a Temp View\n",
    "output.createOrReplaceTempView(\"peoples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cc392d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-------+--------------------+\n",
      "|    name|age|friends|           insert_ts|\n",
      "+--------+---+-------+--------------------+\n",
      "|Jean-Luc| 26|      2|2023-09-24 13:27:...|\n",
      "|    Hugh| 27|    181|2023-09-24 13:27:...|\n",
      "|  Weyoun| 22|    323|2023-09-24 13:27:...|\n",
      "|   Miles| 19|    268|2023-09-24 13:27:...|\n",
      "|  Julian| 25|      1|2023-09-24 13:27:...|\n",
      "+--------+---+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Running a simple Spark SQL query\n",
    "spark.sql(\"select name,age,friends,insert_ts from peoples\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4eba473",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5df07c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"MissingValues.csv\", header=True, inferSchema=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599a0c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Abdul|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|      Ali|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Sheraz|  22|         1| 15000|\n",
      "|    Ahmed|  23|         2| 18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|\n",
      "|     NULL|  34|        10| 38000|\n",
      "|     NULL|  36|      NULL|  NULL|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76e8baf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Abdul| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|      Ali| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Sheraz| 22|         1| 15000|\n",
      "|    Ahmed| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# thresh = 3      means minimum 3 Non-Null values\n",
    "# how = any/all   means drop a row if it contains any/all nulls.\n",
    "df.na.drop(how='all',thresh=3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3022ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Abdul| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|      Ali| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Sheraz| 22|         1| 15000|\n",
      "|    Ahmed| 23|         2| 18000|\n",
      "|     NULL| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset = list,tuple,columns\n",
    "df.na.drop(subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcca2d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Abdul| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|      Ali| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Sheraz| 22|         1| 15000|\n",
      "|    Ahmed| 23|         2| 18000|\n",
      "|   Mahesh| 10|      NULL| 40000|\n",
      "|  Missing| 34|        10| 38000|\n",
      "|  Missing| 36|      NULL|  NULL|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fillinf Missing Values\n",
    "df.na.fill({\"Name\":\"Missing\",\"age\":10}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c6ee6668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['age', 'Experience', 'Salary'], \n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['age', 'Experience', 'Salary']]\n",
    "    ).setStrategy(\"mean\")\n",
    "# Same for median and mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d8795698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Abdul|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|      Ali|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Sheraz|  22|         1| 15000|         22|                 1|         15000|\n",
      "|    Ahmed|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|NULL|      NULL| 40000|         28|                 5|         40000|\n",
      "|     NULL|  34|        10| 38000|         34|                10|         38000|\n",
      "|     NULL|  36|      NULL|  NULL|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d5d163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
